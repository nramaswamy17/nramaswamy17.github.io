---
title: "Neural RRT*: Explained"
date: 2025-08-13
permalink: /posts/2025/08/neural-rrt-star-explained/
categories: motion-planning
tags:
  - algorithms
  - path-planning
  - robotics
  - neural-networks
  - machine-learning
  - computer-science
---


## Article Goal
Explain Neural RRT* (Neural Rapidly-exploring Random Tree Star) algorithm for learning-based optimal path planning

## What is Neural RRT*?
**Neural RRT*** is a learning-based optimal path planning algorithm that combines Convolutional Neural Networks (CNNs) with the traditional RRT* algorithm to achieve faster convergence to optimal solutions. Neural RRT* addresses the key limitations of traditional RRT* - sensitivity to initial solutions and slow convergence - by using a pre-trained CNN model to predict probability distributions for optimal paths, enabling intelligent non-uniform sampling instead of random exploration.

The algorithm replaces RRT*'s uniform random sampling with a learned sampling distribution that biases exploration toward regions more likely to contain optimal paths. This neural guidance significantly reduces the time and computational resources needed to find high-quality solutions.

**[Neural RRT*: Written Walkthrough](/files/Neural_RRT.pdf)**

## Core Concepts
1. **Learned Sampling Distribution** - Uses a CNN to predict probability maps that guide where to sample next, replacing uniform random sampling.
2. **Non-Uniform Intelligent Sampling** - Samples are drawn from areas with higher probability of containing optimal paths rather than uniformly across the space.
3. **CNN-Based Path Prediction** - A pre-trained neural network analyzes the environment and predicts likely regions for optimal paths.
4. **Hybrid Approach** - Combines the asymptotic optimality guarantees of RRT* with the efficiency of learned heuristics.

## The Math

### Traditional RRT* Sampling
In standard RRT*, samples are drawn uniformly:
$$x_{\text{rand}} \sim \text{Uniform}(\mathcal{X}_{\text{free}})$$

### Neural RRT* Sampling Distribution
Neural RRT* uses a learned probability distribution:
$$x_{\text{rand}} \sim P_{\theta}(x | \text{map}, x_{\text{start}}, x_{\text{goal}})$$

Where $$P_{\theta}$$ is the CNN-predicted probability distribution parameterized by $$\theta$$.

### CNN Architecture
The neural network takes as input:
$$\text{Input} = [\text{Map}, \text{Start}, \text{Goal}]$$

And outputs a probability heatmap:
$$\text{Output} = P_{\theta}(x, y | \text{Input}) \in [0,1]^{H \times W}$$

### Probability Normalization
The output probability map is normalized to ensure valid sampling:
$$P_{\text{norm}}(x,y) = \frac{P_{\theta}(x,y)}{\sum_{i,j} P_{\theta}(i,j)}$$

### Training Loss Function
The CNN is typically trained using successful paths from A* or other optimal planners:
$$\mathcal{L} = -\sum_{i=1}^{N} \sum_{(x,y) \in \text{path}_i} \log P_{\theta}(x,y | \text{map}_i, s_i, g_i)$$

Where $$N$$ is the number of training examples and $$\text{path}_i$$ represents ground truth optimal paths.

### Hybrid Sampling Strategy
Neural RRT* often uses a mixture of learned and uniform sampling:
$$x_{\text{rand}} = \begin{cases} 
P_{\theta}(\cdot) & \text{with probability } \alpha \\
\text{Uniform}(\mathcal{X}_{\text{free}}) & \text{with probability } 1-\alpha
\end{cases}$$

## Algorithm Steps
1. **Pre-Training Phase** - Train CNN on dataset of optimal paths generated by A* or other complete planners
2. **Initialize Tree** - Start RRT* tree with initial configuration
3. **Neural Prediction** - Use trained CNN to generate probability heatmap for current environment
4. **Intelligent Sampling** - Sample new configurations from learned probability distribution
5. **Traditional RRT* Operations** - Perform standard nearest neighbor, steer, and collision check operations
6. **Tree Extension** - Add collision-free nodes to tree with rewiring for optimality
7. **Adaptive Sampling** - Optionally update probability distribution as tree grows
8. **Convergence** - Repeat until goal reached or computational budget exhausted



## Advantages of Neural RRT*
1. **Faster Convergence** - Reaches near-optimal solutions significantly faster than traditional RRT*
2. **Reduced Sample Complexity** - Requires fewer random samples due to intelligent guidance
3. **Environment Generalization** - Pre-trained models can adapt to new similar environments
4. **Maintains Optimality** - Preserves asymptotic optimality guarantees of RRT*
5. **Parallel Training** - CNN training can be done offline and reused across problems
6. **Reduced Computational Waste** - Avoids sampling in obviously suboptimal regions

## Limitations
1. **Training Data Dependency** - Requires large datasets of optimal solutions for effective training
2. **Environment Specificity** - CNN may not generalize well to environments significantly different from training data
3. **Discretization Effects** - Performance depends on resolution of probability heatmaps
4. **Training Overhead** - Initial CNN training requires significant computational resources
5. **Memory Requirements** - Must store neural network parameters and probability maps
